#!/usr/bin/env node

import { readFile } from 'fs/promises';
import { Logger, BigQueryHelper, loadSqlTemplate } from './utils.js';

async function createExternalTable(config, bq, tableName, sourceUri, schema) {
  Logger.info(`Creating external table: ${tableName}`);

  // Convert schema to BigQuery DDL format
  const schemaFields = schema.map(field => `${field.name} ${field.type}`).join(',\n  ');

  const createTableSql = `
    CREATE OR REPLACE EXTERNAL TABLE \`${config.project}.${config.dataset}.${tableName}\`
    (
      ${schemaFields}
    )
    OPTIONS (
      format = 'CSV',
      uris = ['${sourceUri}'],
      field_delimiter = '\\t',
      quote = '',
      allow_jagged_rows = true,
      allow_quoted_newlines = false,
      max_bad_records = ${config.validation.expectedErrors || 0}
    )
  `;

  await bq.executeQuery(createTableSql);
  Logger.success(`External table created: ${tableName}`);
}

async function loadRawData(config, bq) {
  const tableName = config.tables.raw;

  if (await bq.tableExists(tableName)) {
    Logger.info(`Raw table ${tableName} already exists; skipping load.`);
    return;
  }

  // Check if schema file exists (generated by prepare step)
  const schemaJson = `${config.paths.tmpDir}/schema.json`;
  let schemaData;
  try {
    schemaData = await readFile(schemaJson, 'utf8');
  } catch (error) {
    if (error.code === 'ENOENT') {
      throw new Error(`Schema file not found: ${schemaJson}. Please run 'npm run prep' first to generate the schema from columns.csv`);
    }
    throw error;
  }
  const schema = JSON.parse(schemaData);

  // Determine source URI - use transformDest if available (preprocessed files), otherwise sourceUri
  const sourceUri = config.gcs.transformDest ?
    config.gcs.transformDest + config.gcs.sourceUri.split('/').pop() :
    config.gcs.sourceUri;

  Logger.info(`About to load from GCS URI: ${sourceUri}`);
  Logger.info(`Schema has ${schema.length} fields`);

  // Check if external tables are enabled
  if (config.pipeline_config.use_external_tables) {
    Logger.info(`Creating external table: ${tableName}`);
    await createExternalTable(config, bq, tableName, sourceUri, schema);
  } else {
    Logger.info(`Loading raw data into native table: ${tableName}`);

    // Load options for Adobe data - critical: disable quoting to prevent column drift
    const loadOptions = {
      fieldDelimiter: '\t',
      quote: '',                    // CRITICAL: disable quoting for Adobe TSV files with stray quotes
      allowJaggedRows: true,       // Reject rows with wrong column counts (count as bad records)
      ignoreUnknownValues: false,   // Don't drop columns - reject malformed rows instead
      allowQuotedNewlines: false,   // Prevent multiline field issues
      maxBadRecords: config.validation.expectedErrors || 0,  // Allow configured number of bad records
      writeDisposition: 'WRITE_TRUNCATE'
    };

    Logger.info(`Allowing up to ${loadOptions.maxBadRecords} bad records during load`);
    await bq.loadTable(tableName, sourceUri, schema, loadOptions);
  }

  // Get row count
  const countSql = `SELECT COUNT(*) as num_rows FROM \`${config.project}.${config.dataset}.${tableName}\``;
  const [rows] = await bq.bq.query(countSql);
  Logger.info(`Loaded ${rows[0].num_rows} rows into ${tableName}`);

  // Data quality validation checks
  await validateRawDataQuality(config, bq, tableName);
}

async function validateRawDataQuality(config, bq, tableName) {
  Logger.info('Running data quality validation checks...');

  // Get basic table statistics
  const statsSql = `SELECT COUNT(*) as total_rows FROM \`${config.project}.${config.dataset}.${tableName}\``;
  const [statsRows] = await bq.bq.query(statsSql);
  const actualRows = parseInt(statsRows[0].total_rows);

  Logger.info(`Loaded ${actualRows} rows into ${tableName}`);

  // Validate against expected row count (if configured)
  if (config.validation.expectedRows > 0) {
    const rowDiff = Math.abs(actualRows - config.validation.expectedRows);
    const percentDiff = (rowDiff / config.validation.expectedRows) * 100;

    if (percentDiff > 10) {
      Logger.warn(`Row count differs significantly from expected: got ${actualRows}, expected ${config.validation.expectedRows} (${percentDiff.toFixed(1)}% difference)`);
    } else {
      Logger.success(`Row count validation passed: ${actualRows} rows (${percentDiff.toFixed(1)}% difference from expected)`);
    }
  }

  // Run raw data quality validation (timestamps and hit IDs)
  Logger.info('Validating timestamp and hit ID integrity...');

  const qualitySql = await loadSqlTemplate('./models/validate-raw-quality.sql', {
    project: config.project,
    dataset: config.dataset,
    rawTable: tableName
  });

  const [qualityRows] = await bq.bq.query(qualitySql);
  const quality = qualityRows[0];

  // Log detailed quality metrics
  Logger.info('Raw data quality analysis:');
  Logger.info(`  ‚Ä¢ Total records: ${quality.total_records.toLocaleString()}`);
  Logger.info(`  ‚Ä¢ Clean records: ${quality.clean_records.toLocaleString()} (${quality.clean_record_pct}%)`);
  Logger.info(`  ‚Ä¢ Malformed records: ${quality.malformed_record_count.toLocaleString()} (${quality.malformed_record_pct}%)`);

  if (quality.malformed_record_count > 0) {
    Logger.info('Quality breakdown:');
    Logger.info(`    - Bad hit_time_gmt: ${quality.malformed_hit_time_gmt.toLocaleString()} (${quality.malformed_hit_time_gmt_pct}%)`);
    Logger.info(`    - Bad hitid_high: ${quality.malformed_hitid_high.toLocaleString()} (${quality.malformed_hitid_high_pct}%)`);
    Logger.info(`    - Bad hitid_low: ${quality.malformed_hitid_low.toLocaleString()} (${quality.malformed_hitid_low_pct}%)`);
  }

  // Issue warnings based on quality thresholds
  const malformedPct = parseFloat(quality.malformed_record_pct);
  if (malformedPct > 5) {
    Logger.warn(`‚ö†Ô∏è  High malformed record rate: ${malformedPct}%. This indicates significant data quality issues.`);
  } else if (malformedPct > 1) {
    Logger.warn(`‚ö†Ô∏è  Moderate malformed record rate: ${malformedPct}%. Some records will be dropped in Bronze transformation.`);
  } else if (malformedPct > 0) {
    Logger.info(`‚ÑπÔ∏è  Low malformed record rate: ${malformedPct}%. Expected data quality variance.`);
  } else {
    Logger.success(`‚úÖ Perfect data quality: 0% malformed records`);
  }

  // Important note about Bronze transformation
  if (quality.malformed_record_count > 0) {
    Logger.info(`üìù Note: ${quality.malformed_record_count.toLocaleString()} malformed records will be filtered out during Bronze transformation.`);
    Logger.info(`   Expected Bronze record count: ~${quality.clean_records.toLocaleString()} (${quality.clean_record_pct}% of raw)`);
  }

  Logger.success('Data quality validation complete');
}

async function loadLookupTable(bq, tableName, filePath, schema, skipHeader = false, extraOptions = {}) {
  if (await bq.tableExists(tableName)) {
    Logger.info(`Table ${tableName} already exists; skipping load.`);
    return;
  }

  Logger.info(`Loading lookup table: ${tableName} from ${filePath}`);

  const loadOptions = {
    fieldDelimiter: '\t',
    quote: '',
    skipLeadingRows: skipHeader ? 1 : 0,
    writeDisposition: 'WRITE_TRUNCATE',
    ...extraOptions  // Allow overriding options
  };

  await bq.loadLocalTable(tableName, filePath, schema, loadOptions);
}

async function loadSDRTable(bq, tableName, filePath, schema) {
  if (await bq.tableExists(tableName)) {
    Logger.info(`SDR table ${tableName} already exists; skipping load.`);
    return;
  }

  Logger.info(`Loading SDR table: ${tableName} from ${filePath}`);

  const loadOptions = {
    fieldDelimiter: '\t',
    quote: '',
    skipLeadingRows: 1,  // SDR files have headers
    ignoreUnknownValues: true,  // Only keep first two columns
    writeDisposition: 'WRITE_TRUNCATE'
  };

  await bq.loadLocalTable(tableName, filePath, schema, loadOptions);
}

export async function load(config) {
  Logger.info('=== Loading Phase ===');
  Logger.info(`Project: ${config.project}`);
  Logger.info(`Dataset: ${config.dataset}`);
  Logger.info(`Raw table: ${config.tables.raw}`);

  const sourceUri = config.gcs.transformDest ?
    config.gcs.transformDest + config.gcs.sourceUri.split('/').pop() :
    config.gcs.sourceUri;
  Logger.info(`GCS URI: ${sourceUri}${config.gcs.transformDest ? ' (preprocessed)' : ' (raw)'}`);
  console.log();

  const bq = new BigQueryHelper(config);
  await bq.ensureDataset();

  // Load raw data
  await loadRawData(config, bq);

  // Load base event lookup (id, name)
  const eventSchema = [
    { name: 'id', type: 'INTEGER', mode: 'NULLABLE' },
    { name: 'name', type: 'STRING', mode: 'NULLABLE' }
  ];
  await loadLookupTable(bq, 'lookup_events', './lookups/events.tsv', eventSchema);

  // Load SDR tables
  const sdrEventSchema = [
    { name: 'Event', type: 'STRING', mode: 'NULLABLE' },
    { name: 'Name', type: 'STRING', mode: 'NULLABLE' }
  ];
  await loadSDRTable(bq, 'sdr_custom_events_raw', './lookups/sdr/custom-events.tsv', sdrEventSchema);

  const sdrEvarSchema = [
    { name: 'EvarNumber', type: 'STRING', mode: 'NULLABLE' },
    { name: 'Name', type: 'STRING', mode: 'NULLABLE' }
  ];
  await loadSDRTable(bq, 'sdr_evars_raw', './lookups/sdr/evars.tsv', sdrEvarSchema);

  const sdrPropSchema = [
    { name: 'PropertyNumber', type: 'STRING', mode: 'NULLABLE' },
    { name: 'Name', type: 'STRING', mode: 'NULLABLE' }
  ];
  await loadSDRTable(bq, 'sdr_props_raw', './lookups/sdr/custom-props.tsv', sdrPropSchema);

  // Load value lookup tables
  const valueLookupFiles = [
    'browser.tsv',
    'browser_type.tsv',
    'connection_type.tsv',
    'languages.tsv',
    'referrer_type.tsv',
    'country.tsv',
    'operating_systems.tsv',
    'resolution.tsv',
    'color_depth.tsv',
    'javascript_version.tsv',
    'plugins.tsv',
    'search_engines.tsv'
  ];

  for (const file of valueLookupFiles) {
    const tableName = `lookup_${file.replace('.tsv', '')}`;
    const filePath = `./lookups/values/${file}`;

    // All value lookup files use the same 2-column schema (id, name)
    // ignoreUnknownValues will drop any extra columns like the 3rd column in referrer_type.tsv
    const lookupOptions = {
      fieldDelimiter: '\t',
      quote: '',
      skipLeadingRows: 0,
      ignoreUnknownValues: true,  // Drop extra columns beyond schema
      allowJaggedRows: true       // Allow rows with fewer columns
    };
    await loadLookupTable(bq, tableName, filePath, eventSchema, false, lookupOptions);
  }

  // Create SDR mapping tables
  Logger.info('Creating SDR mapping tables...');
  if (await bq.tableExists('sdr_custom_events') &&
      await bq.tableExists('sdr_evars') &&
      await bq.tableExists('event_map')) {
    Logger.info('SDR mapping tables already exist; skipping creation.');
  } else {
    const sdrSql = await loadSqlTemplate('./models/create-sdr-maps.sql', {
      project: config.project,
      dataset: config.dataset
    });
    await bq.executeQuery(sdrSql);
    Logger.success('SDR mapping tables created');
  }

  // Generate comprehensive load summary
  await generateLoadSummary(config, bq);

  Logger.success('Loading phase complete');
}

async function generateLoadSummary(config, bq) {
  Logger.info('=== Load Summary ===');

  const tablesToCheck = [
    // Main data table
    { name: config.tables.raw, type: 'Raw Data', description: 'Adobe Analytics source data' },

    // Base lookup tables
    { name: 'lookup_events', type: 'Events', description: 'Base event codes and names' },

    // SDR tables
    { name: 'sdr_custom_events_raw', type: 'SDR Raw', description: 'Custom events mapping (raw)' },
    { name: 'sdr_evars_raw', type: 'SDR Raw', description: 'eVars mapping (raw)' },
    { name: 'sdr_props_raw', type: 'SDR Raw', description: 'Props mapping (raw)' },
    { name: 'sdr_custom_events', type: 'SDR Processed', description: 'Custom events mapping (processed)' },
    { name: 'sdr_evars', type: 'SDR Processed', description: 'eVars mapping (processed)' },
    { name: 'event_map', type: 'SDR Processed', description: 'Unified event mapping' },

    // Value lookup tables
    { name: 'lookup_browser', type: 'Lookup', description: 'Browser lookup' },
    { name: 'lookup_browser_type', type: 'Lookup', description: 'Browser type lookup' },
    { name: 'lookup_connection_type', type: 'Lookup', description: 'Connection type lookup' },
    { name: 'lookup_languages', type: 'Lookup', description: 'Languages lookup' },
    { name: 'lookup_referrer_type', type: 'Lookup', description: 'Referrer type lookup' },
    { name: 'lookup_country', type: 'Lookup', description: 'Country lookup' },
    { name: 'lookup_operating_systems', type: 'Lookup', description: 'Operating systems lookup' },
    { name: 'lookup_resolution', type: 'Lookup', description: 'Resolution lookup' },
    { name: 'lookup_color_depth', type: 'Lookup', description: 'Color depth lookup' },
    { name: 'lookup_javascript_version', type: 'Lookup', description: 'JavaScript version lookup' },
    { name: 'lookup_plugins', type: 'Lookup', description: 'Plugins lookup' },
    { name: 'lookup_search_engines', type: 'Lookup', description: 'Search engines lookup' }
  ];

  const loadedTables = [];
  const missingTables = [];
  let totalRows = 0;

  // Check each table
  for (const table of tablesToCheck) {
    try {
      if (await bq.tableExists(table.name)) {
        // Get row count
        const countSql = `SELECT COUNT(*) as num_rows FROM \`${config.project}.${config.dataset}.${table.name}\``;
        const [rows] = await bq.bq.query(countSql);
        const rowCount = parseInt(rows[0].num_rows);

        loadedTables.push({
          ...table,
          rowCount
        });

        if (table.type === 'Raw Data') {
          totalRows = rowCount;
        }
      } else {
        missingTables.push(table);
      }
    } catch (error) {
      missingTables.push({ ...table, error: error.message });
    }
  }

  // Display summary by category
  const categories = ['Raw Data', 'SDR Raw', 'SDR Processed', 'Events', 'Lookup'];

  for (const category of categories) {
    const categoryTables = loadedTables.filter(t => t.type === category);
    if (categoryTables.length > 0) {
      Logger.info(`\nüìä ${category} Tables:`);
      for (const table of categoryTables) {
        const formattedCount = table.rowCount.toLocaleString();
        Logger.success(`  ‚úÖ ${table.name}: ${formattedCount} rows - ${table.description}`);
      }
    }
  }

  // Summary statistics
  console.log();
  Logger.info('üìà Summary Statistics:');
  Logger.success(`  ‚Ä¢ Total main data rows: ${totalRows.toLocaleString()}`);
  Logger.success(`  ‚Ä¢ Tables loaded: ${loadedTables.length}`);
  Logger.success(`  ‚Ä¢ Lookup tables: ${loadedTables.filter(t => t.type === 'Lookup').length}`);
  Logger.success(`  ‚Ä¢ SDR mapping tables: ${loadedTables.filter(t => t.type.includes('SDR')).length}`);

  // Data source information
  const sourceUri = config.gcs.transformDest ?
    config.gcs.transformDest + config.gcs.sourceUri.split('/').pop() :
    config.gcs.sourceUri;
  Logger.info(`  ‚Ä¢ Data source: ${sourceUri}${config.gcs.transformDest ? ' (preprocessed)' : ' (raw)'}`);

  // Warning for missing tables
  if (missingTables.length > 0) {
    console.log();
    Logger.warn(`‚ö†Ô∏è  Missing or failed tables (${missingTables.length}):`);
    for (const table of missingTables) {
      const errorMsg = table.error ? ` (${table.error})` : '';
      Logger.warn(`  ‚Ä¢ ${table.name}${errorMsg}`);
    }
  }

  console.log();
}

// Allow running as standalone script
if (import.meta.url === `file://${process.argv[1]}`) {
  try {
    const configData = await readFile('./config.json', 'utf8');
    const config = JSON.parse(configData);
    await load(config);
  } catch (error) {
    Logger.error(`Loading failed: ${error.message}`);
    process.exit(1);
  }
}